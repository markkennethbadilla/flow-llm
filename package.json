{
    "name": "flow-llm",
    "version": "1.0.0",
    "description": "Semantic caching layer for LLM APIs using local vector embeddings",
    "main": "src/FlowLLM.tsx",
    "scripts": {
        "test": "echo \"Error: no test specified\" && exit 1"
    },
    "keywords": [
        "ai",
        "semantic-cache",
        "vector-search",
        "embeddings",
        "llm"
    ],
    "author": "Mark Kenneth Badilla",
    "license": "MIT",
    "dependencies": {
        "@xenova/transformers": "^2.15.0",
        "framer-motion": "^11.0.0",
        "lucide-react": "^0.300.0",
        "react-markdown": "^9.0.0",
        "remark-gfm": "^4.0.0",
        "react": "^18.2.0",
        "react-dom": "^18.2.0"
    },
    "devDependencies": {
        "@types/node": "^20.0.0",
        "@types/react": "^18.2.0",
        "@types/react-dom": "^18.2.0",
        "typescript": "^5.0.0"
    }
}